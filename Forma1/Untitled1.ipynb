{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "This notebooks uses the file generated by the notebook `process_dates_screen_storage.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import Area Under the Receiver Operating Characteristic Curve metric to evaluate results\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data...\n",
    "The files loaded are generated by the notebook `process_dates_screen_starge.ipynb`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train_df_processed_screenResol_storage_dates.csv')\n",
    "\n",
    "to_predict = pd.read_csv('../data/to_predict_processed_screenResol_storage_dates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Note on preprocessing\n",
    "All preprocessing which can be done in just one way, i.e. it doesn't need hyper parameter adjustment, will be done outside pipelines and then stored to a new file, so there will be no need to execute the same code every time we open this notebook again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good pipeline resources: \n",
    "* http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "* https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines\n",
    "* https://www.kaggle.com/metadist/work-like-a-pro-with-pipelines-and-feature-unions\n",
    "* https://www.kaggle.com/sermakarevich/sklearn-pipelines-tutorial\n",
    "* http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build some custom transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature pipeline creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.33\n",
    "# define a seed, so same experiments output same results every time and experiments between them become comparable\n",
    "seed = 12\n",
    "\n",
    "# realizo train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df.loc[:, train_df.columns != 'label'], \n",
    "                                                    train_df.label, \n",
    "                                                    test_size=test_size, \n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassure object types are all strings, because some processings fails otherwise. Apparently there is some value which is not a string...\n",
    "X_train.model = X_train.model.apply(lambda x: str(x))\n",
    "X_test.model = X_test.model.apply(lambda x: str(x))\n",
    "train_df.model = train_df.model.apply(lambda x: str(x))\n",
    "\n",
    "X_train.color = X_train.color.apply(lambda x: str(x))\n",
    "X_test.color = X_test.color.apply(lambda x: str(x))\n",
    "train_df.color = train_df.color.apply(lambda x: str(x))\n",
    "\n",
    "X_train.search_term = X_train.search_term.apply(lambda x: str(x))\n",
    "X_test.search_term = X_test.search_term.apply(lambda x: str(x))\n",
    "train_df.search_term = train_df.search_term.apply(lambda x: str(x))\n",
    "\n",
    "X_train.url = X_train.url.apply(lambda x: str(x))\n",
    "X_test.url = X_test.url.apply(lambda x: str(x))\n",
    "train_df.url = train_df.url.apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "import tensorflow as tf\n",
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper\n",
    "\n",
    "auc_roc = as_keras_metric(tf.metrics.auc)\n",
    "recall = as_keras_metric(tf.metrics.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# from https://rdrr.io/github/rstudio/keras/man/sequences_to_matrix.html\n",
    "# sequences_to_matrix(tokenizer, sequences, mode = c(\"binary\", \"count\", \"tfidf\", \"freq\"))\n",
    "tokenizer.fit_on_texts(X_train.search_term)\n",
    "X_train.search_term = tokenizer.texts_to_matrix(X_train.search_term,mode='tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_categorical_transformer = Pipeline([\n",
    "    (\"imputer\",SimpleImputer(strategy='most_frequent')),\n",
    "    (\"one_hot\",OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "large_categorical_transformer = Pipeline([\n",
    "    (\"imputer\",SimpleImputer(strategy='most_frequent')),\n",
    "    (\"hashing_trick\",FeatureHasher(input_type='string'))\n",
    "])\n",
    "\n",
    "tf_idf = Pipeline([\n",
    "#     (\"imputer\",SimpleImputer(strategy='constant',fill_value=\"\")),\n",
    "#     ('vect',CountVectorizer(ngram_range=(1,1), binary=True, min_df=3,lowercase=False)),\n",
    "#     ('tfidf', TfidfTransformer())\n",
    "    (\"tf_idf\",TfidfVectorizer()),\n",
    "#     ('best', TruncatedSVD())\n",
    "])\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "#     (\"large_cat\",large_categorical_transformer,[\"person\",\"skus\",\"city\",\"region\",\"country\"]),\n",
    "#     (\"tf_idf\",FeatureHasher(n_features=30,input_type='string'),['model','color','search_term','url'])\n",
    "#     (\"tf_idf\",tf_idf,['color'])\n",
    "]\n",
    "    ,n_jobs=-1\n",
    "    ,remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.fillna(0, inplace=True)\n",
    "X_test.fillna(0, inplace=True)\n",
    "to_predict.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A host of Scikit-learn models\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "SEED = seed\n",
    "\n",
    "def get_models():\n",
    "    \"\"\"Generate a library of base learners.\"\"\"\n",
    "    nb = GaussianNB()\n",
    "    svc = SVC(C=100, probability=True)\n",
    "    knn = KNeighborsClassifier(n_neighbors=10,metric='chebyshev')\n",
    "    lr = LogisticRegression(solver='saga',C=100, random_state=SEED, n_jobs=-1)\n",
    "    nn = MLPClassifier((80, 10), early_stopping=True, random_state=SEED)\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, random_state=SEED)\n",
    "    xgb = XGBClassifier()\n",
    "    ada = AdaBoostClassifier()\n",
    "    bag = BaggingClassifier(n_jobs=-1)\n",
    "    extra_t = ExtraTreesClassifier(n_jobs=-1)\n",
    "    dec_tree = DecisionTreeClassifier()\n",
    "    ext_tree = ExtraTreeClassifier()\n",
    "    sgd = SGDClassifier(n_jobs=-1, loss='log')\n",
    "    rf = RandomForestClassifier(n_estimators=10, max_features=3, random_state=SEED, n_jobs=-1)\n",
    "\n",
    "    models = {'svm': svc,\n",
    "              'knn': knn,\n",
    "              'naive bayes': nb,\n",
    "              'mlp-nn': nn,\n",
    "              'random forest': rf,\n",
    "              'gbm': gb,\n",
    "              'xgb': xgb,\n",
    "              'ada': ada,\n",
    "              'bag': bag,\n",
    "              'extra_tree': extra_t,\n",
    "              'dec_tree': dec_tree,\n",
    "              'ext_tree': ext_tree,\n",
    "              'sgd': sgd,\n",
    "              'logistic': lr,\n",
    "              }\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "def train_predict(model_list):\n",
    "    \"\"\"Fit models in list on training set and return preds\"\"\"\n",
    "    P = np.zeros((y_test.shape[0], len(model_list)))\n",
    "    P = pd.DataFrame(P)\n",
    "\n",
    "    print(\"Fitting models.\")\n",
    "    cols = list()\n",
    "    for i, (name, m) in enumerate(models.items()):\n",
    "        print(\"%s...\" % name, end=\" \", flush=False)\n",
    "        m.fit(X_train.drop(['model','color','search_term','url','person','skus','city','region','country'],axis=1), y_train)\n",
    "        P.iloc[:, i] = m.predict_proba(X_test.drop(['model','color','search_term','url','person','skus','city','region','country'],axis=1))[:, 1]\n",
    "        cols.append(name)\n",
    "        print(\"done\")\n",
    "\n",
    "    P.columns = cols\n",
    "    print(\"Done.\\n\")\n",
    "    return P\n",
    "\n",
    "\n",
    "def score_models(P, y):\n",
    "    \"\"\"Score model in prediction DF\"\"\"\n",
    "    print(\"Scoring models.\")\n",
    "    for m in P.columns:\n",
    "        score = roc_auc_score(y, P.loc[:, m])\n",
    "        print(\"%-26s: %.3f\" % (m, score))\n",
    "    print(\"Done.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = get_models()\n",
    "P = train_predict(models)\n",
    "score_models(P, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learners = get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_learner = GradientBoostingClassifier(\n",
    "    n_estimators=1000,\n",
    "    loss=\"exponential\",\n",
    "    max_features=4,\n",
    "    max_depth=3,\n",
    "    subsample=0.5,\n",
    "    learning_rate=0.005, \n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_base_learners(base_learners, inp, out, verbose=True):\n",
    "    \"\"\"Train all base learners in the library.\"\"\"\n",
    "    if verbose: print(\"Fitting models.\")\n",
    "    for i, (name, m) in enumerate(base_learners.items()):\n",
    "        if verbose: print(\"%s...\" % name, end=\" \", flush=False)\n",
    "        m.fit(inp, out)\n",
    "        if verbose: print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_base, xpred_base, ytrain_base, ypred_base = train_test_split(\n",
    "    X_train.drop(['model','color','search_term','url','person','skus','city','region','country'],axis=1), y_train, test_size=0.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_base_learners(base_learners, xtrain_base, ytrain_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_base_learners(pred_base_learners, inp, verbose=True):\n",
    "    \"\"\"Generate a prediction matrix.\"\"\"\n",
    "    P = np.zeros((inp.shape[0], len(pred_base_learners)))\n",
    "\n",
    "    if verbose: print(\"Generating base learner predictions.\")\n",
    "    for i, (name, m) in enumerate(pred_base_learners.items()):\n",
    "        if verbose: print(\"%s...\" % name, end=\" \", flush=False)\n",
    "        p = m.predict_proba(inp)\n",
    "        # With two classes, need only predictions for one class\n",
    "        P[:, i] = p[:, 1]\n",
    "        if verbose: print(\"done\")\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_base = predict_base_learners(base_learners, xpred_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_learner.fit(P_base, ypred_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(base_learners, meta_learner, inp, verbose=True):\n",
    "    \"\"\"Generate predictions from the ensemble.\"\"\"\n",
    "    P_pred = predict_base_learners(base_learners, inp, verbose=verbose)\n",
    "    return P_pred, meta_learner.predict_proba(P_pred)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_pred, p = ensemble_predict(base_learners, meta_learner, X_test.drop(['model','color','search_term','url','person','skus','city','region','country'],axis=1))\n",
    "print(\"\\nEnsemble ROC-AUC score: %.3f\" % roc_auc_score(y_test, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "def stacking(base_learners, meta_learner, X, y, generator):\n",
    "    \"\"\"Simple training routine for stacking.\"\"\"\n",
    "\n",
    "    # Train final base learners for test time\n",
    "    print(\"Fitting final base learners...\", end=\"\")\n",
    "    train_base_learners(base_learners, X, y, verbose=False)\n",
    "    print(\"done\")\n",
    "\n",
    "    # Generate predictions for training meta learners\n",
    "    # Outer loop:\n",
    "    print(\"Generating cross-validated predictions...\")\n",
    "    cv_preds, cv_y = [], []\n",
    "    for i, (train_idx, test_idx) in enumerate(generator.split(X)):\n",
    "\n",
    "        fold_xtrain, fold_ytrain = X[train_idx, :], y[train_idx]\n",
    "        fold_xtest, fold_ytest = X[test_idx, :], y[test_idx]\n",
    "\n",
    "        # Inner loop: step 4 and 5\n",
    "        fold_base_learners = {name: clone(model)\n",
    "                              for name, model in base_learners.items()}\n",
    "        train_base_learners(\n",
    "            fold_base_learners, fold_xtrain, fold_ytrain, verbose=False)\n",
    "\n",
    "        fold_P_base = predict_base_learners(\n",
    "            fold_base_learners, fold_xtest, verbose=False)\n",
    "\n",
    "        cv_preds.append(fold_P_base)\n",
    "        cv_y.append(fold_ytest)\n",
    "        print(\"Fold %i done\" % (i + 1))\n",
    "\n",
    "    print(\"CV-predictions done\")\n",
    "    \n",
    "    # Be careful to get rows in the right order\n",
    "    cv_preds = np.vstack(cv_preds)\n",
    "    cv_y = np.hstack(cv_y)\n",
    "\n",
    "    # Train meta learner\n",
    "    print(\"Fitting meta learner...\", end=\"\")\n",
    "    meta_learner.fit(cv_preds, cv_y)\n",
    "    print(\"done\")\n",
    "\n",
    "    return base_learners, meta_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Train with stacking\n",
    "cv_base_learners, cv_meta_learner = stacking(\n",
    "    get_models(), clone(meta_learner), X_train.drop(['model','color','search_term','url','person','skus','city','region','country'],axis=1).values, y_train.values, KFold(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_pred, p = ensemble_predict(cv_base_learners, cv_meta_learner, X_test.drop(['model','color','search_term','url','person','skus','city','region','country'],axis=1).values, verbose=False)\n",
    "print(\"\\nEnsemble ROC-AUC score: %.3f\" % roc_auc_score(y_test, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_processing = Pipeline([\n",
    "    ('preproc', ct),\n",
    "    ('predict', XGBClassifier())\n",
    "#     ('svm',SVC())\n",
    "#     ('sgd', linear_model.SGDClassifier())\n",
    "]\n",
    "#     ,memory=cachedir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_processing.fit(X_train.drop(['model','color','search_term','url','person','skus','city','region','country'],axis=1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = feature_processing.predict_proba(X_test.drop(['model','color','search_term','url','person','skus','city','region','country'],axis=1))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape of predictions\n",
    "if (preds.shape == y_test.shape):\n",
    "    print('shapes OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev: 0.8886417505271881\n",
    "#       0.7675200441464977\n",
    "#       0.8501359056263651\n",
    "#       0.9994019683472557    logistic_regression con preprocesamiento de browswer y os.\n",
    "#       0.8501359056263651    idem pero con xgb\n",
    "#       0.8503577934604342    xgb con:\n",
    "#                                     ct = ColumnTransformer([\n",
    "#                                         (\"lim_cat\",limited_categorical_transformer,[\"event\",\"condition\",\"staticpage\",\n",
    "#                                                                                     \"campaign_source\",\"search_engine\",\n",
    "#                                                                                     \"channel\",\"new_vs_returning\",\"device_type\",\n",
    "#                                                                                     \"operating_system_version\",\"browser_version\"]),\n",
    "#                                         (\"large_cat\",large_categorical_transformer,[\"person\",\"url\",\"skus\",\"city\",\"region\",\"country\"]),\n",
    "#                                         (\"tf_idf\",TfidfVectorizer(),\"model\"),\n",
    "#                                         (\"tf_idf2\",TfidfVectorizer(),\"color\"),\n",
    "#                                         (\"tf_idf_reduced\",TfidfVectorizer(),\"search_term\"),\n",
    "#                                         (\"passthrough\",'passthrough',[\"storage\",\"sku\",\"year\",\"month_sin\",\"month_cos\",\"day_sin\",\"day_cos\",\"weekday_sin\",\n",
    "#                                                                       \"weekday_cos\",\"hour_sin\",\"hour_cos\",\"screen_width\",\"screen_height\"])\n",
    "#                                     ],n_jobs=-1)\n",
    "#       0.8492173710139364    [best kaggle score]-> 0.84537 \n",
    "#                             xgclassifier solamente con features numericos de nuevos superdf de 100 y pico de features. \n",
    "roc_auc_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_pred, p = ensemble_predict(cv_base_learners, cv_meta_learner, to_predict.drop(['model','color','search_term','url','person','skus','city','region','country'],axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_posta = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first train with whole dataset\n",
    "feature_processing.fit(train_df.drop(['model','color','search_term','url','person','skus','city','region','country','label'],axis=1), train_df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_posta = feature_processing.predict_proba(to_predict.drop(['model','color','search_term','url','person','skus','city','region','country'],axis=1))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_posta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_publish = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_publish['person'] = to_predict.person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_publish['label'] = preds_posta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_publish.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_publish.groupby('person', as_index=False).mean().to_csv('../predictions/6.dic@00.05.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
