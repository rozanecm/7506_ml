{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "This notebooks uses the file generated by the notebook `process_dates_screen_storage.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import Area Under the Receiver Operating Characteristic Curve metric to evaluate results\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data...\n",
    "The files loaded are generated by the notebook `process_dates_screen_starge.ipynb`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train_df_processed_screenResol_storage_dates.csv')\n",
    "\n",
    "to_predict = pd.read_csv('../data/to_predict_processed_screenResol_storage_dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassure object types are all strings, because some processings fails otherwise. Apparently there is some value which is not a string...\n",
    "X_train.model = X_train.model.apply(lambda x: str(x))\n",
    "X_test.model = X_test.model.apply(lambda x: str(x))\n",
    "\n",
    "X_train.color = X_train.color.apply(lambda x: str(x))\n",
    "X_test.color = X_test.color.apply(lambda x: str(x))\n",
    "\n",
    "X_train.search_term = X_train.search_term.apply(lambda x: str(x))\n",
    "X_test.search_term = X_test.search_term.apply(lambda x: str(x))\n",
    "\n",
    "X_train.url = X_train.url.apply(lambda x: str(x))\n",
    "X_test.url = X_test.url.apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Note on preprocessing\n",
    "All preprocessing which can be done in just one way, i.e. it doesn't need hyper parameter adjustment, will be done outside pipelines and then stored to a new file, so there will be no need to execute the same code every time we open this notebook again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good pipeline resources: \n",
    "* http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "* https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines\n",
    "* https://www.kaggle.com/metadist/work-like-a-pro-with-pipelines-and-feature-unions\n",
    "* https://www.kaggle.com/sermakarevich/sklearn-pipelines-tutorial\n",
    "* http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build some custom transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature pipeline creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.33\n",
    "# define a seed, so same experiments output same results every time and experiments between them become comparable\n",
    "seed = 12\n",
    "\n",
    "# realizo train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df.loc[:, train_df.columns != 'label'], \n",
    "                                                    train_df.label, \n",
    "                                                    test_size=test_size, \n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['person', 'url', 'model', 'color', 'skus', 'search_term', 'city',\n",
       "       'region', 'country'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_categorical_transformer = Pipeline([\n",
    "    (\"imputer\",SimpleImputer(strategy='most_frequent')),\n",
    "    (\"one_hot\",OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_categorical_transformer = Pipeline([\n",
    "    (\"imputer\",SimpleImputer(strategy='most_frequent')),\n",
    "    (\"hashing_trick\",FeatureHasher(input_type='string'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = Pipeline([\n",
    "#     (\"imputer\",SimpleImputer(strategy='constant',fill_value=\"\")),\n",
    "#     ('vect',CountVectorizer(ngram_range=(1,1), binary=True, min_df=3,lowercase=False)),\n",
    "#     ('tfidf', TfidfTransformer())\n",
    "    (\"tf_idf\",TfidfVectorizer()),\n",
    "#     ('best', TruncatedSVD())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([\n",
    "    (\"large_cat\",large_categorical_transformer,[\"person\",\"skus\",\"city\",\"region\",\"country\"]),\n",
    "#     (\"tf_idf\",FeatureHasher(n_features=30,input_type='string'),['model','color','search_term','url'])\n",
    "#     (\"tf_idf\",tf_idf,['color'])\n",
    "]\n",
    "    ,n_jobs=-1\n",
    "#     ,remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.color.fillna(\"\", inplace=True)\n",
    "# X_train.model.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import mkdtemp\n",
    "cachedir = mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_processing = Pipeline([\n",
    "    ('preproc', ct),\n",
    "    ('predict', XGBClassifier())\n",
    "    ]))\n",
    "],memory=cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='/tmp/tmpzbqjdrbw',\n",
       "     steps=[('preproc', ColumnTransformer(n_jobs=-1, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('large_cat', Pipeline(memory=None,\n",
       "     steps=[('imputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "       strategy='most_frequent',...obs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_processing.fit(X_train.drop(['model','color','search_term','url'],axis=1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = feature_processing.predict_proba(X_test.drop(['model','color','search_term','url'],axis=1))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes OK\n"
     ]
    }
   ],
   "source": [
    "# check shape of predictions\n",
    "if (preds.shape == y_test.shape):\n",
    "    print('shapes OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49791264437895877"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prev: 0.8886417505271881\n",
    "#       0.7675200441464977\n",
    "#       0.8501359056263651\n",
    "#       0.9994019683472557    logistic_regression con preprocesamiento de browswer y os.\n",
    "#       0.8501359056263651    idem pero con xgb\n",
    "#       0.8503577934604342    xgb con:\n",
    "#                                     ct = ColumnTransformer([\n",
    "#                                         (\"lim_cat\",limited_categorical_transformer,[\"event\",\"condition\",\"staticpage\",\n",
    "#                                                                                     \"campaign_source\",\"search_engine\",\n",
    "#                                                                                     \"channel\",\"new_vs_returning\",\"device_type\",\n",
    "#                                                                                     \"operating_system_version\",\"browser_version\"]),\n",
    "#                                         (\"large_cat\",large_categorical_transformer,[\"person\",\"url\",\"skus\",\"city\",\"region\",\"country\"]),\n",
    "#                                         (\"tf_idf\",TfidfVectorizer(),\"model\"),\n",
    "#                                         (\"tf_idf2\",TfidfVectorizer(),\"color\"),\n",
    "#                                         (\"tf_idf_reduced\",TfidfVectorizer(),\"search_term\"),\n",
    "#                                         (\"passthrough\",'passthrough',[\"storage\",\"sku\",\"year\",\"month_sin\",\"month_cos\",\"day_sin\",\"day_cos\",\"weekday_sin\",\n",
    "#                                                                       \"weekday_cos\",\"hour_sin\",\"hour_cos\",\"screen_width\",\"screen_height\"])\n",
    "#                                     ],n_jobs=-1)\n",
    "#       0.8492173710139364    [best kaggle score]-> 0.84537 \n",
    "#                             xgclassifier solamente con features numericos de nuevos superdf de 100 y pico de features. \n",
    "roc_auc_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_processing.named_steps['preproc'].named_transformers_['tf_idf'].idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_posta = feature_processing.predict_proba(to_predict.drop(['model','color','search_term','url'],axis=1))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03057893, 0.07978979, 0.03293054, ..., 0.07043858, 0.11072487,\n",
       "       0.00669352], dtype=float32)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_posta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_publish = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_publish['person'] = to_predict.person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_publish['label'] = preds_posta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19415, 2)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_publish.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_publish.groupby('person', as_index=False).mean().to_csv('../predictions/2.dic@02.50.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
